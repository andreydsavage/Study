{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports and requirements\n",
    "\n",
    "* В данном соревновании мы имеем дело с последовательностями, один из интуитивных способов работы с ними - использование рекуррентных сетей. Данный бейзлайн посвящен тому, чтобы показать, как можно строить хорошие решения без использования сложного и трудоемкого feature engineering-а (чтобы эффективно решать ту же задачу с высоким качеством с помощью бустингов нужно несколько тысяч признаков), благодаря рекуррентным сетям. В этом ноутбуке мы построим решение с использованием фреймфорка `torch`. Для комфортной работы Вам понадобится машина с `GPU` (хватит ресурсов `google colab` или `kaggle`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# добавим корневую папку, в ней лежат все необходимые полезные функции для обработки данных\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocecc transactions\n",
    "\n",
    "transactions = pd.read_csv(PATH + 'transactions.csv', parse_dates=['transaction_dttm'])\n",
    "clients = pd.read_csv(PATH + 'clients.csv')\n",
    "report_dates = pd.read_csv(PATH + 'report_dates.csv',parse_dates=['report_dt'])\n",
    "clients= clients.merge(report_dates, how='left', on='report')\n",
    "\n",
    "transactions = transactions.merge(clients[['user_id', 'report_dt']], how='left', on='user_id')\n",
    "\n",
    "transactions['transaction_dttm'] = transactions['transaction_dttm'].astype('datetime64[ns]')\n",
    "transactions['dweek'] = transactions['transaction_dttm'].dt.dayofweek\n",
    "transactions['date'] = transactions['transaction_dttm'].dt.date.astype('datetime64[ns]')\n",
    "transactions['year'] = transactions['transaction_dttm'].dt.year\n",
    "transactions['year'] = transactions['year'].apply(lambda x: 1 if x == 2022 else 2)\n",
    "transactions['month'] = transactions['transaction_dttm'].dt.month\n",
    "transactions['hour'] = transactions['transaction_dttm'].dt.hour\n",
    "transactions['week'] = transactions['transaction_dttm'].dt.isocalendar()['week']\n",
    "\n",
    "transactions['days'] = (transactions['report_dt']-transactions['transaction_dttm']).dt.days\n",
    "transactions['days_dif'] = transactions['days'].diff().fillna(0).abs()\n",
    "\n",
    "transactions.head()\n",
    "\n",
    "# Убрем 0 из данных\n",
    "for col in transactions.columns:\n",
    "    min = transactions[col].min()\n",
    "    print(col, 'наимньшее', min)\n",
    "    if min == 0:\n",
    "        transactions[col] = transactions[col] + 1 \n",
    "        min = transactions[col].min()\n",
    "        print(col, 'наимньшее', min)\n",
    "\n",
    "transactions.to_csv(PATH+'transactions_mod.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['user_id', 'mcc_code', 'currency_rk', 'transaction_amt',\n",
    "       'transaction_dttm', 'report_dt', 'dweek', 'date', 'year', 'month',\n",
    "       'hour', 'week', 'days', 'days_dif']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Как и в случае с бустингами, мы не можем поместить всю выборку в память, в виду, например, ограниченных ресурсов. Для итеративного чтения данных нам потребуется функция `utils.read_parquet_dataset_from_local`, которая читает N частей датасета за раз в память.\n",
    "\n",
    "\n",
    "* Нейронные сети требуют отделнього внимания к тому, как будут поданы и обработаны данные. Важные моменты, на которые требуется обратить внимание:\n",
    "    * Использование рекуррентных сетей подразумевает работу на уровне последовательностей, где одна последовательность - все исторические транзакции клиента. Чтобы преобразовать `pd.DataFrame` с транзакциями клиентов в табличном виде к последовательностям, мы подготовили функцию `dataset_preprocessing_utils.transform_transactions_to_sequences`, она делает необходимые манипуляции и возвращает фрейм с двумя колонками: `app_id` и `sequences`. Колонка `sequence` представляет из себя массив массивов длины `len(features)`, где каждый вложенный массив - значения одного конкретного признака во всех транзакциях клиента. \n",
    "    \n",
    "    * каждый клиент имеет различную по длине историю транзакций. При этом обучение сетей происходит батчами, что требует делать паддинги в последовательностях. Довольно неэффективно делать паддинг внутри батча на последовательностях случайной длины (довольно часто будем делать большой и бесполезный паддинг). Гораздо лучше использовать технику `sequence_bucketing` (о ней рассказано в образовательном ролике к данному бейзлайну). Для этого мы предоставляем функцию `dataset_preprocessing_utils.create_padded_buckets`. Один из аргументов в данную функцию - `bucket_info` - словарь, где для конкретной длины последовательности указано до какой длины нужно делать паддинг. Мы предоставялем для старта простой вид разбиения на 100 бакетов и файл где лежит отображение каждой длины в падднг (файл `buckets_info.pkl`).\n",
    "    \n",
    "    * Такие признаки, как [`amnt`, `days_before`, `hour_diff`] по своей природе не являются категориальными. Вы в праве самостоятельно выбирать способ работы с ними (модифицируя функции бейзлайна или адаптируя под себя). В рамках бейзлайна мы предлагаем интерпретировать каждую не категориальную фичу как категориальную. Для этого нужно подготовить bin-ы для каждой фичи. Мы предлагаем простой способ разбиения по бинам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import read_parquet_dataset_from_local\n",
    "from dataset_preprocessing_utils import transform_transactions_to_sequences, create_padded_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../buckets_info.pkl', 'rb') as f:\n",
    "    mapping_seq_len_to_padded_len = pickle.load(f)\n",
    "    \n",
    "with open('../dense_features_buckets.pkl', 'rb') as f:\n",
    "    dense_features_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_seq_len_to_padded_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make bucket info\n",
    "\n",
    "# mapping_seq = seq['sequence_length'].quantile([0.25,0.5,0.75]).values\n",
    "\n",
    "# mapping_seq= mapping_seq.astype(int)\n",
    "\n",
    "# mapping = {}\n",
    "# for sequence in seq['sequence_length'].values:\n",
    "#     if sequence <= mapping_seq[0]:\n",
    "#         mapping[sequence] = mapping_seq[0]\n",
    "#     elif sequence <= mapping_seq[1]:\n",
    "#         mapping[sequence] = mapping_seq[1]\n",
    "#     else:\n",
    "#         mapping[sequence] = mapping_seq[2]\n",
    "    \n",
    "# mapping_seq_len_to_padded_len = mapping\n",
    "\n",
    "# with open('../buckets_info.pkl', 'wb') as f:\n",
    "#     pickle.dump(mapping_seq_len_to_padded_len, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make dense_features_buckets\n",
    "\n",
    "# dense_features_buckets = {'transaction_amt':[-2407.71005859,  -981.33168597,  -558.5010376 ,  -359.30341666,\n",
    "#         -230.8977805 ,  -141.41950509,   -65.03555559,   0]}\n",
    "\n",
    "# with open('../dense_features_buckets.pkl', 'wb') as f:\n",
    "#     pickle.dump(dense_features_buckets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions['transaction_amt'].quantile(np.linspace(0.1,0.95, 8)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(PATH + '/train.csv')\n",
    "# seq = seq.merge(train, how='left', on='user_id')\n",
    "# seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[    1     2     3 ... 63996 63998 63999]\n",
      "[    0    11    29 ... 63990 63993 63997]\n",
      "*************************\n",
      "1\n",
      "[    0     1     3 ... 63996 63997 63999]\n",
      "[    2     4     5 ... 63985 63989 63998]\n",
      "*************************\n",
      "2\n",
      "[    0     1     2 ... 63997 63998 63999]\n",
      "[    7     8    16 ... 63963 63966 63991]\n",
      "*************************\n",
      "3\n",
      "[    0     2     3 ... 63997 63998 63999]\n",
      "[    1     6    10 ... 63988 63994 63996]\n",
      "*************************\n",
      "4\n",
      "[    0     1     2 ... 63996 63997 63998]\n",
      "[    3    12    15 ... 63992 63995 63999]\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train = pd.read_csv(PATH + '/train.csv')\n",
    "\n",
    "for split, (train_index, valid_index) in enumerate(strat_kfold.split(train, train['target'])):\n",
    "    X_train, X_val = train.iloc[train_index], train.iloc[valid_index]\n",
    "    print(split)\n",
    "    print(train_index)\n",
    "    print(valid_index)\n",
    "    print('*'*25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63978</th>\n",
       "      <td>559523</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63984</th>\n",
       "      <td>560300</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63992</th>\n",
       "      <td>561033</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63995</th>\n",
       "      <td>561824</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63999</th>\n",
       "      <td>562740</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  target  time\n",
       "3           41       0    57\n",
       "12          76       0    78\n",
       "15          82       0    91\n",
       "17          94       0    91\n",
       "21         140       0    91\n",
       "...        ...     ...   ...\n",
       "63978   559523       0    74\n",
       "63984   560300       0    91\n",
       "63992   561033       0    91\n",
       "63995   561824       0    91\n",
       "63999   562740       0    91\n",
       "\n",
       "[12800 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63978</th>\n",
       "      <td>559523</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63984</th>\n",
       "      <td>560300</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63992</th>\n",
       "      <td>561033</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63995</th>\n",
       "      <td>561824</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63999</th>\n",
       "      <td>562740</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  target  time\n",
       "3           41       0    57\n",
       "12          76       0    78\n",
       "15          82       0    91\n",
       "17          94       0    91\n",
       "21         140       0    91\n",
       "...        ...     ...   ...\n",
       "63978   559523       0    74\n",
       "63984   560300       0    91\n",
       "63992   561033       0    91\n",
       "63995   561824       0    91\n",
       "63999   562740       0    91\n",
       "\n",
       "[12800 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./processed_chunk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_buckets_from_transactions(path = PATH, save_to_path = './processed_chunk/'):\n",
    "\n",
    "    transactions_frame = pd.read_csv(path + 'transactions_mod.csv', parse_dates=['transaction_dttm'])\n",
    "    for dense_col in ['transaction_amt']:\n",
    "        transactions_frame[dense_col] = np.digitize(transactions_frame[dense_col], bins=dense_features_buckets[dense_col])\n",
    "        \n",
    "    seq = transform_transactions_to_sequences(transactions_frame)\n",
    "    seq['sequence_length'] = seq.sequences.apply(lambda x: len(x[1]))\n",
    "\n",
    "    train = pd.read_csv(path + '/train.csv')\n",
    "    seq = seq.merge(train, how='left', on='user_id')\n",
    "    seq_train = seq[seq['target'].notna()]\n",
    "    seq_test = seq[seq['target'].isna()]\n",
    "\n",
    "    # for split in range(5):\n",
    "    #     random_state = 10*split\n",
    "    #     X_train, X_val = train_test_split(seq_train, random_state=random_state, test_size=0.25)\n",
    "    strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for split, (train_index, valid_index) in enumerate(strat_kfold.split(seq_train, seq_train['target'])):\n",
    "        X_train, X_val = seq_train.iloc[train_index], seq_train.iloc[valid_index]\n",
    "\n",
    "        processed_train =  create_padded_buckets(X_train, mapping_seq_len_to_padded_len, has_target=True, \n",
    "                                                    save_to_file_path=os.path.join(save_to_path, \n",
    "                                                                                f'processed_chunk_train_{split}.pkl'))\n",
    "        processed_valid = create_padded_buckets(X_val, mapping_seq_len_to_padded_len, has_target=True, \n",
    "                                                    save_to_file_path=os.path.join(save_to_path, \n",
    "                                                                                f'processed_chunk_valid_{split}.pkl'))\n",
    "        \n",
    "    processed_test =  create_padded_buckets(seq_test, mapping_seq_len_to_padded_len, has_target=False, \n",
    "                                                save_to_file_path=os.path.join(save_to_path, \n",
    "                                                                            f'processed_chunk_test.pkl'))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # for split, (train_index, valid_index) in enumerate(strat_kfold.split(seq_train, seq_train['target'])):\n",
    "    #     X_train, X_val = seq_train.iloc[train_index], seq_train.iloc[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b4a25ee1b74fc0bf6f53b9460455a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cff9ee27d64519bbee876f13f55697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ed90b7ea9943ee9b7b285f98a2dcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e9d3391ca44351824f169fd40fbbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc5367ba9354786a59d0feb23d0c5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b4bd066f18457484f2b7175e6d99ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa9bba11271473892fd07eca53b22e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc275456bc994d7d8cd3e7b986d9cf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8fb78186294748b539dbc6a1094470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e3037947044d8882c99d80d948a958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n",
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences['bucket_idx'] = 186 #frame_of_sequences.sequence_length.map(bucket_info)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9908b02bd42b4f7d9bd49760e5b1fbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting buckets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/andrey/HDD/Study/ods_data_fusion_2024/rnn_andvanced/baseline/../dataset_preprocessing_utils.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frame_of_sequences.drop(columns=['bucket_idx'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "create_buckets_from_transactions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Разобьем имеющиеся данные на `train` и `val` части. Воспользуемся самым простым способом - для валидации используем 10% случайных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для создания модели будем использовать фреймворк `torch`. В нем есть все, чтобы писать произвольные сложные архитектуры и быстро эксперементировать. Для того, чтобы мониторить и логировать весь процесс во время обучения сетей, рекомендуется использовать надстройки над данным фреймворков, например, `lightning`.\n",
    "\n",
    "* В бейзлайне мы предлагаем базовые компоненты, чтобы можно было обучать нейронную сеть и отслеживать ее качество. Для этого вам предоставлены следующие функции:\n",
    "    * `data_generators.batches_generator` - функция-генератор, итеративно возвращает батчи, поддерживает батчи для `tensorflow.keras` и `torch.nn.module` моделей. В зависимости от флага `is_train` может быть использована для генерации батчей на train/val/test стадию.\n",
    "    * функция `pytorch_training.train_epoch` - обучает модель одну эпоху.\n",
    "    * функция `pytorch_training.eval_model` - проверяет качество модели на отложенной выборке и возвращает roc_auc_score.\n",
    "    * функция `pytorch_training.inference` - делает предикты на новых данных и готовит фрейм для проверяющей системы.\n",
    "    * класс `training_aux.EarlyStopping` - реализует early_stopping, сохраняя лучшую модель. Пример использования приведен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import batches_generator, transaction_features\n",
    "from pytorch_training import train_epoch, eval_model, inference\n",
    "from training_aux import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Все признаки в нашей модели будут категориальными. Для их представления в модели используем категориальные эмбеддинги. Для этого нужно каждому категориальному признаку задать размерность латентного пространства. Используем [формулу](https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608) из библиотеки `fast.ai`. Все отображения хранятся в файле `embedding_projections.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../embedding_projections.pkl', 'rb') as f:\n",
    "#     embedding_projections = pickle.load(f)\n",
    "\n",
    "# embedding_projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id unique 562740\n",
      "mcc_code unique 450\n",
      "currency_rk unique 4\n",
      "transaction_amt unique 352076.8125\n",
      "transaction_dttm unique 2023-03-20 20:59:58\n",
      "report_dt unique 2023-06-30 03:00:00\n",
      "dweek unique 7\n",
      "date unique 2023-03-20\n",
      "year unique 2023\n",
      "month unique 12\n",
      "hour unique 24\n",
      "week unique 52\n",
      "days unique 283\n",
      "days_dif unique 183.0\n"
     ]
    }
   ],
   "source": [
    "for col in transactions.columns:\n",
    "    print(col, 'unique', transactions[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mcc_code',\n",
       " 'currency_rk',\n",
       " 'transaction_amt',\n",
       " 'dweek',\n",
       " 'year',\n",
       " 'month',\n",
       " 'hour',\n",
       " 'week',\n",
       " 'days',\n",
       " 'days_dif']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "So it looks like it changed since I last looked. Previously, it was:\n",
    "\n",
    "def emb_sz_rule(n_cat:int)->int: return min(50, (n_cat//2)+1)\n",
    "\n",
    "Now it looks like it’s the following:\n",
    "\n",
    "def emb_sz_rule(n_cat:int)->int: return min(600, round(1.6 * n_cat**0.56))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_projections = {'mcc_code': (450, 200), 'currency_rk': (4,3), 'transaction_amt': (10,6),\n",
    "                         'dweek': (7,4), 'year':(2,2), 'month':(12,7), 'hour':(24,13), 'week': (52,27),  'days': (283,141), 'days_dif':(183,92)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Реализуем модель. Все входные признаки представим в виде эмбеддингов, сконкатенируем, чтобы получить векторное представление транзакции. Подадим последовательности в `GRU` рекуррентную сеть. Используем последнее скрытое состояние в качестве выхода сети. Представим признак `product` в виде отдельного эмбеддинга. Сконкатенируем его с выходом сети. На основе такого входа построим небольшой `MLP`, выступающий классификатором для целевой задачи. Используем градиентный спуск, чтобы решить оптимизационную задачу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionsRnn(nn.Module):\n",
    "    def __init__(self, transactions_cat_features, embedding_projections, product_col_name='product', rnn_units=128, top_classifier_units=32):\n",
    "        super(TransactionsRnn, self).__init__()\n",
    "        self._transaction_cat_embeddings = nn.ModuleList([self._create_embedding_projection(*embedding_projections[feature]) \n",
    "                                                          for feature in transactions_cat_features])\n",
    "                \n",
    "        # self._product_embedding = self._create_embedding_projection(*embedding_projections[product_col_name], padding_idx=None)\n",
    "        \n",
    "        self._gru = nn.GRU(input_size=sum([embedding_projections[x][1] for x in transactions_cat_features]),\n",
    "                             hidden_size=rnn_units, batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self._hidden_size = rnn_units\n",
    "                \n",
    "        self._top_classifier = nn.Linear(in_features=rnn_units, \n",
    "                                         out_features=top_classifier_units)\n",
    "        self._intermediate_activation = nn.ReLU()\n",
    "        \n",
    "        self._head = nn.Linear(in_features=top_classifier_units, out_features=1)\n",
    "    \n",
    "    def forward(self, transactions_cat_features):\n",
    "        batch_size = 32\n",
    "        \n",
    "        embeddings = [embedding(transactions_cat_features[i]) for i, embedding in enumerate(self._transaction_cat_embeddings)]\n",
    "        concated_embeddings = torch.cat(embeddings, dim=-1)\n",
    "        \n",
    "        _, last_hidden = self._gru(concated_embeddings)\n",
    "        last_hidden = torch.reshape(last_hidden.permute(1, 2, 0), shape=(batch_size, self._hidden_size))\n",
    "        \n",
    "        # product_embed = self._product_embedding(product_feature)\n",
    "        \n",
    "        intermediate_concat = torch.cat([last_hidden], dim=-1)\n",
    "                \n",
    "        classification_hidden = self._top_classifier(intermediate_concat)\n",
    "        activation = self._intermediate_activation(classification_hidden)\n",
    "        \n",
    "        logit = self._head(activation)\n",
    "        \n",
    "        return logit\n",
    "    \n",
    "    @classmethod\n",
    "    def _create_embedding_projection(cls, cardinality, embed_size, add_missing=True, padding_idx=0):\n",
    "        add_missing = 1 if add_missing else 0\n",
    "        return nn.Embedding(num_embeddings=cardinality+add_missing, embedding_dim=embed_size, padding_idx=padding_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./checkpoints/’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir ./checkpoints/\n",
    "\n",
    "! rm -r ./checkpoints/pytorch_baseline/\n",
    "! mkdir ./checkpoints/pytorch_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./checkpoints/kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для того, чтобы детектировать переобучение используем EarlyStopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoints = './checkpoints/kfold/'\n",
    "es = EarlyStopping(patience=3, mode='max', verbose=True, save_path=os.path.join(path_to_checkpoints, 'best_checkpoint.pt'), \n",
    "                   metric_name='ROC-AUC', save_format='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "train_batch_size = 32\n",
    "val_batch_szie = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(f'./processed_chunk/processed_chunk_train_{i}.pkl', 'rb') as f:\n",
    "        dataset_train = pickle.load(f)  \n",
    "    with open(f'./processed_chunk/processed_chunk_valid_{i}.pkl', 'rb') as f:\n",
    "        dataset_valid = pickle.load(f)     \n",
    "        \n",
    "    model = TransactionsRnn(transaction_features, embedding_projections).to(device)\n",
    "    optimizer = torch.optim.Adam(lr=1e-3, params=model.parameters())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        train_epoch(model, optimizer, dataset_train, batch_size=train_batch_size, \n",
    "                    print_loss_every_n_batches=100, device=device)\n",
    "        \n",
    "        val_roc_auc = eval_model(model, dataset_valid, batch_size=val_batch_szie, device=device)\n",
    "        # es(val_roc_auc, model)\n",
    "        \n",
    "        # if es.early_stop:\n",
    "        #     print('Early stopping reached. Stop training...')\n",
    "        #     break\n",
    "        torch.save(model.state_dict(), os.path.join(path_to_checkpoints, f'epoch_{epoch+1}_val_{val_roc_auc:.3f}_split_{i}.pt'))\n",
    "        \n",
    "        train_roc_auc = eval_model(model, dataset_train, batch_size=val_batch_szie, device=device)\n",
    "        print(f'Epoch {epoch+1} completed. Train roc-auc: {train_roc_auc}, Val roc-auc: {val_roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Запустим цикл обучения, каждую эпоху будем логировать лосс, а так же roc-auc на валидации и на обучении. Будем сохрнаять веса после каждой эпохи, а так же лучшие с помощью early_stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Все готово, чтобы сделать предсказания для тестовой выборки. Нужно только подготовить данные в том же формате, как и для train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ../../../test_buckets\n",
    "! mkdir ../../../test_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Отдельный вопрос, какую из построенных моделей использовать для того, чтобы делать предсказания на тест. Можно выбирать лучшую по early_stopping. В таком случае есть риск, что мы подгонимся под валидационную выборку, особенно если она не является очень репрезентативной, однако это самый базовый вариант (используем его). Можно делать разные версии ансамблирования, используя веса с разных эпох. Такой подход требует дополнительного кода (обязательно попробуйте его!). Наконец, можно выбирать такую модель, которая показывает хорошие результаты на валидации и в то же время, не слишком переучена под train выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_1_val_0.649_split_3.pt  epoch_2_val_0.672_split_1.pt\n",
      "epoch_1_val_0.665_split_1.pt  epoch_2_val_0.681_split_2.pt\n",
      "epoch_1_val_0.671_split_0.pt  epoch_3_val_0.655_split_3.pt\n",
      "epoch_1_val_0.673_split_4.pt  epoch_3_val_0.657_split_1.pt\n",
      "epoch_1_val_0.682_split_2.pt  epoch_3_val_0.660_split_0.pt\n",
      "epoch_2_val_0.654_split_3.pt  epoch_3_val_0.667_split_4.pt\n",
      "epoch_2_val_0.665_split_0.pt  epoch_3_val_0.673_split_2.pt\n",
      "epoch_2_val_0.669_split_4.pt\n"
     ]
    }
   ],
   "source": [
    "! ls $path_to_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./checkpoints/kfold/epoch_2_val_0.654_split_3.pt',\n",
       " './checkpoints/kfold/epoch_2_val_0.681_split_2.pt',\n",
       " './checkpoints/kfold/epoch_2_val_0.672_split_1.pt',\n",
       " './checkpoints/kfold/epoch_2_val_0.665_split_0.pt',\n",
       " './checkpoints/kfold/epoch_2_val_0.669_split_4.pt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob(path_to_checkpoints + 'epoch_2*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(5):\n",
    "    with open(f'./processed_chunk_valid_{i}.pkl', 'rb') as f:\n",
    "        dataset_valid = pickle.load(f) \n",
    "\n",
    "    sample = pd.read_csv(PATH + 'sample_submit_naive.csv')\n",
    "    sample['predict'] = 0\n",
    "    out = np.zeros(16000)\n",
    "    for i in glob.glob(path_to_checkpoints + 'epoch_2*'):\n",
    "        model.load_state_dict(torch.load(i))\n",
    "        test_preds = inference(model, dataset_valid, batch_size=32, device=device)\n",
    "        \n",
    "        out += test_preds['predict'].values\n",
    "        score.append(metrics.roc_auc_score(dataset_valid['targets'][0], out/5))\n",
    "\n",
    "\n",
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.031792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>0.025088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>0.030441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>0.088427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   predict\n",
       "0        9  0.031792\n",
       "1       61  0.025088\n",
       "2       62  0.003125\n",
       "3       80  0.030441\n",
       "4       88  0.088427"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5232ab89f6354b06b328bce063b610c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test time predictions: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980bbd2c499f4b8c8313e7737b426f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test time predictions: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30e51d75f954bcebca42e1fc705ca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test time predictions: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e773d3bd064407aa1e729d939a2e930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test time predictions: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5459fcd435ad40389446024f9ea227a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test time predictions: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.298615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>0.777377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>0.236845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>0.655628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>0.928605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>561362</td>\n",
       "      <td>0.247617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>561419</td>\n",
       "      <td>0.394992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>561895</td>\n",
       "      <td>0.589917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>561908</td>\n",
       "      <td>0.768405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>562205</td>\n",
       "      <td>0.711206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   predict\n",
       "0            9  0.298615\n",
       "1           61  0.777377\n",
       "2           62  0.236845\n",
       "3           80  0.655628\n",
       "4           88  0.928605\n",
       "...        ...       ...\n",
       "31995   561362  0.247617\n",
       "31996   561419  0.394992\n",
       "31997   561895  0.589917\n",
       "31998   561908  0.768405\n",
       "31999   562205  0.711206\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(f'./processed_chunk_test.pkl', 'rb') as f:\n",
    "    dataset_test = pickle.load(f) \n",
    "\n",
    "sample = pd.read_csv(PATH + 'sample_submit_naive.csv')\n",
    "sample['predict'] = 0\n",
    "for i in glob.glob(path_to_checkpoints + 'epoch_2*'):\n",
    "    model.load_state_dict(torch.load(i))\n",
    "    test_preds = inference(model, dataset_test, batch_size=32, device=device)\n",
    "    \n",
    "    sample['predict'] =  sample['predict'] +test_preds['predict'].values\n",
    "\n",
    "sample['predict'] = sample['predict']\n",
    "\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.059723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>0.155475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>0.047369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>0.131126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>0.185721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>561362</td>\n",
       "      <td>0.049523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>561419</td>\n",
       "      <td>0.078998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>561895</td>\n",
       "      <td>0.117983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>561908</td>\n",
       "      <td>0.153681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>562205</td>\n",
       "      <td>0.142241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   predict\n",
       "0            9  0.059723\n",
       "1           61  0.155475\n",
       "2           62  0.047369\n",
       "3           80  0.131126\n",
       "4           88  0.185721\n",
       "...        ...       ...\n",
       "31995   561362  0.049523\n",
       "31996   561419  0.078998\n",
       "31997   561895  0.117983\n",
       "31998   561908  0.153681\n",
       "31999   562205  0.142241\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['predict'] = sample['predict']\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('rnn_baseline_submission.csv', index=None) # ~ 0.750 на public test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../../baseline/submit_baseline_0.770.csv')\n",
    "submit\n",
    "# submit['predict'] = 0.7*submit['predict']+0.2*sample['predict'] # 0.775\n",
    "submit['predict'] = 0.9*submit['predict']+0.1*sample['predict'] # 0.775\n",
    "# submit['predict'] = 0.5*submit['predict']+0.5*test_preds['predict'] # 0.771\n",
    "\n",
    "submit.to_csv('rnn_baseline_stacked_09_01.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.082403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>0.113720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>0.243039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>0.037259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>0.611515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>561362</td>\n",
       "      <td>0.278235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>561419</td>\n",
       "      <td>0.279356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>561895</td>\n",
       "      <td>0.217220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>561908</td>\n",
       "      <td>0.329889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>562205</td>\n",
       "      <td>0.237406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   predict\n",
       "0            9  0.082403\n",
       "1           61  0.113720\n",
       "2           62  0.243039\n",
       "3           80  0.037259\n",
       "4           88  0.611515\n",
       "...        ...       ...\n",
       "31995   561362  0.278235\n",
       "31996   561419  0.279356\n",
       "31997   561895  0.217220\n",
       "31998   561908  0.329889\n",
       "31999   562205  0.237406\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
